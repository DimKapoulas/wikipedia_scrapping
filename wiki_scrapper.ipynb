{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fea88e9-8fa7-43c0-8aef-782ccef5d792",
   "metadata": {},
   "source": [
    "# Scrapping for revenue\n",
    "\n",
    "![Web Scraping](https://t.ly/9KRqy)\n",
    "\n",
    "This is a notebook showcasing some simple scraping using the [Beautifulsoup](https://pypi.org/project/beautifulsoup4/) and [Requests](https://pypi.org/project/requests/) modules for Python.\n",
    "\n",
    "The data is related with the Revenue of the Largest Companies in the US and extracted from the following wikipedia link,\n",
    "[here](https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue)\n",
    "\n",
    "\n",
    "<img src=\"https://t.ly/SDrMp\" width=\"30%\">\n",
    "\n",
    "First of all we will load our libraries and hit the site to get the page as whole. Pass the url to our `requests()` function and it does what the name implies, getting us the page itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1229dc2-3d37-4a41-9a78-97d6a49ec32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13f48fd-2e5e-4657-8dc5-bd0fcb2e4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the wikipedia page containg the list\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_largest_companies_in_the_United_States_by_revenue'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d430f6-f971-4603-b339-5b264bee3e09",
   "metadata": {},
   "source": [
    "# Extracting the Revenue table\n",
    "\n",
    "We will extract the first table and make a *soup* object out of it for further edit.\n",
    "If we inspect the page, using developer tools, we will see that tables on the page are contained within the <table> html tag and with same css class of 'wikitalbe sortable' which will fetch with a simple `soup.find(<tag_in_question>)` command will do the trick. \n",
    "This simple command search and fetch the first and only the first `<tag>` that was give as its arguement, fortunately for us our table is the very first one in the page so that will do the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cfe6e68-a565-444f-91b4-b98f9735299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the table containing the revenue sorting\n",
    "table = soup.find('table', class_='wikitable sortable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8f035-970c-4818-9498-8f67a0bf9909",
   "metadata": {},
   "source": [
    "# Getting the colunm titles of the table\n",
    "\n",
    "Having our table isolated as a *soup* object we will extract its column headers in the same fashion but this time using the `.find_all()` command because the Headers row contains multiple values.\n",
    "Afterwards, since raw html is not the most user friendly format for the human, we will strip the uncessary tags symbols to turn it into a list of strings for inspection and future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399828cb-b0c5-4878-a44c-06058c56ef38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rank',\n",
       " 'Name',\n",
       " 'Industry',\n",
       " 'Revenue (USD millions)',\n",
       " 'Revenue growth',\n",
       " 'Employees',\n",
       " 'Headquarters']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the table headers, drop html tags and special symbols\n",
    "table_headers = table.find_all('th')\n",
    "titles = [title.text.strip() for title in table_headers]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935ec98-1dfc-40c8-a822-1c9611dd80e9",
   "metadata": {},
   "source": [
    "# Into a Dataframe\n",
    "\n",
    "Our next move is to turni this headers list into a Dataframe which will enable us further manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45982803-2b8a-4d6f-a246-4df805d8c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b1742-1948-4b6e-820d-db769341d8d7",
   "metadata": {},
   "source": [
    "# Extracting the row data of the table\n",
    "\n",
    "After the headers are set we follow a similar approach as above to extract\n",
    "the actual data of the following rows. Again we create a suitable *soup* object, we iterate over it to create an extra individual *soup* object out of them so-that we can easily access their text to append to the previous Dataframe.\n",
    "Finally we reset our index to be the 'Rank' column, preserving the original visual format as in the wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cb232b-3e1d-44f5-9408-270110de4906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Revenue (USD millions)</th>\n",
       "      <th>Revenue growth</th>\n",
       "      <th>Employees</th>\n",
       "      <th>Headquarters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Retail</td>\n",
       "      <td>611,289</td>\n",
       "      <td>6.7%</td>\n",
       "      <td>2,100,000</td>\n",
       "      <td>Bentonville, Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Retail and Cloud Computing</td>\n",
       "      <td>513,983</td>\n",
       "      <td>9.4%</td>\n",
       "      <td>1,540,000</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>Petroleum industry</td>\n",
       "      <td>413,680</td>\n",
       "      <td>44.8%</td>\n",
       "      <td>62,000</td>\n",
       "      <td>Spring, Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Electronics industry</td>\n",
       "      <td>394,328</td>\n",
       "      <td>7.8%</td>\n",
       "      <td>164,000</td>\n",
       "      <td>Cupertino, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>324,162</td>\n",
       "      <td>12.7%</td>\n",
       "      <td>400,000</td>\n",
       "      <td>Minnetonka, Minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Best Buy</td>\n",
       "      <td>Retail</td>\n",
       "      <td>46,298</td>\n",
       "      <td>10.6%</td>\n",
       "      <td>71,100</td>\n",
       "      <td>Richfield, Minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bristol-Myers Squibb</td>\n",
       "      <td>Pharmaceutical industry</td>\n",
       "      <td>46,159</td>\n",
       "      <td>0.5%</td>\n",
       "      <td>34,300</td>\n",
       "      <td>New York City, New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>United Airlines</td>\n",
       "      <td>Airline</td>\n",
       "      <td>44,955</td>\n",
       "      <td>82.5%</td>\n",
       "      <td>92,795</td>\n",
       "      <td>Chicago, Illinois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Thermo Fisher Scientific</td>\n",
       "      <td>Laboratory instruments</td>\n",
       "      <td>44,915</td>\n",
       "      <td>14.5%</td>\n",
       "      <td>130,000</td>\n",
       "      <td>Waltham, Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Qualcomm</td>\n",
       "      <td>Technology</td>\n",
       "      <td>44,200</td>\n",
       "      <td>31.7%</td>\n",
       "      <td>51,000</td>\n",
       "      <td>San Diego, California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name                    Industry  \\\n",
       "Rank                                                         \n",
       "1                      Walmart                      Retail   \n",
       "2                       Amazon  Retail and Cloud Computing   \n",
       "3                  Exxon Mobil          Petroleum industry   \n",
       "4                        Apple        Electronics industry   \n",
       "5           UnitedHealth Group                  Healthcare   \n",
       "...                        ...                         ...   \n",
       "96                    Best Buy                      Retail   \n",
       "97        Bristol-Myers Squibb     Pharmaceutical industry   \n",
       "98             United Airlines                     Airline   \n",
       "99    Thermo Fisher Scientific      Laboratory instruments   \n",
       "100                   Qualcomm                  Technology   \n",
       "\n",
       "     Revenue (USD millions) Revenue growth  Employees             Headquarters  \n",
       "Rank                                                                            \n",
       "1                   611,289           6.7%  2,100,000    Bentonville, Arkansas  \n",
       "2                   513,983           9.4%  1,540,000      Seattle, Washington  \n",
       "3                   413,680          44.8%     62,000            Spring, Texas  \n",
       "4                   394,328           7.8%    164,000    Cupertino, California  \n",
       "5                   324,162          12.7%    400,000    Minnetonka, Minnesota  \n",
       "...                     ...            ...        ...                      ...  \n",
       "96                   46,298          10.6%     71,100     Richfield, Minnesota  \n",
       "97                   46,159           0.5%     34,300  New York City, New York  \n",
       "98                   44,955          82.5%     92,795        Chicago, Illinois  \n",
       "99                   44,915          14.5%    130,000   Waltham, Massachusetts  \n",
       "100                  44,200          31.7%     51,000    San Diego, California  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the table rows, then  the data in them.\n",
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for row in table_rows[1:]:\n",
    "    row_data = row.find_all('td')\n",
    "    indiv_row_data = [data.text.strip() for data in row_data]\n",
    "    \n",
    "    df.loc[len(df)] = indiv_row_data\n",
    "    \n",
    "df.set_index('Rank', inplace=True)    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c22f0-fe8f-4b92-8144-1683255b290b",
   "metadata": {},
   "source": [
    "# Saving the data\n",
    "\n",
    "Lastly we save the data into CSV format which is suitable for processing outside of Python, in other application like Ms Excell ect ect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f7fb12-6993-408a-a09d-54b844f1145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe into a CSV, overwriting existing ones\n",
    "df.to_csv(r'companies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346da94-b237-44f4-953e-aeaf09ba2d0b",
   "metadata": {},
   "source": [
    "# Done!\n",
    "\n",
    "This concludes our scraping. Though simple at its core, it puts the basis for automatition basics as this either can be writen in a `.py` script format and deployed into a web server so it can automatically launch periodically, via a *cronjob* and update the data concerning the revenue. As long as the html format of the table does not change our script will run smoothly, but even if this occurs we can always make use of `try ... expect` statements and notify us via email or other method but until that happens if Walmart happens to fall from its revenue \"crown\", we will probably be the first to notice without visiting wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data311",
   "language": "python",
   "name": "data311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
